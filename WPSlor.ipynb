{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jazzblazzer/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/jazzblazzer/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import torch.nn as nn\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "nltk.download('stopwords')\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['document', 'summary'],\n",
      "        num_rows: 3803957\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['document', 'summary'],\n",
      "        num_rows: 189651\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['document', 'summary'],\n",
      "        num_rows: 1951\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"gigaword\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'document': \"australia 's current account deficit shrunk by a record #.## billion dollars -lrb- #.## billion us -rrb- in the june quarter due to soaring commodity prices , figures released monday showed .\", 'summary': 'australian current account deficit narrows sharply'}\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_TOKEN = \"<s>\"\n",
    "END_TOKEN = \"</s>\"\n",
    "UNKNOWN_TOKEN = \"<unk>\"\n",
    "PAD_TOKEN = \"<pad>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['<s>', 'japan', \"'s\", 'nec', 'corp', 'and', 'unk', 'computer', 'corp', 'of', 'the', 'united', 'states', 'said', 'wednesday', 'they', 'had', 'agreed', 'to', 'join', 'forces', 'in', 'supercomputer', 'sales'], ['japan', \"'s\", 'nec', 'corp', 'and', 'unk', 'computer', 'corp', 'of', 'the', 'united', 'states', 'said', 'wednesday', 'they', 'had', 'agreed', 'to', 'join', 'forces', 'in', 'supercomputer', 'sales', '</s>'])\n"
     ]
    }
   ],
   "source": [
    "class LSTMDataset:\n",
    "    def __init__(self, dataset, tokenizer, max_length):\n",
    "        self.dataset = dataset['document']\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        tokenized_sentence = self.tokenizer(re.sub(r'[^\\w\\s\\']', '',self.dataset[idx].lower()))\n",
    "        return [START_TOKEN]+tokenized_sentence,tokenized_sentence+[END_TOKEN]\n",
    "\n",
    "        \n",
    "    \n",
    "    \n",
    "lstm_data = LSTMDataset(dataset['test'],nltk.word_tokenize, 100)\n",
    "print(lstm_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_train_data = []\n",
    "for i in range(len(lstm_data)):\n",
    "    embed_train_data.append(lstm_data[i][0]+[END_TOKEN])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(embed_train_data, vector_size=300, window=5, min_count=3, workers=4)\n",
    "\n",
    "model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = Word2Vec.load(\"word2vec.model\")\n",
    "vocab = embedding_model.wv.key_to_index\n",
    "#change this to random later\n",
    "vocab[PAD_TOKEN] = len(vocab)\n",
    "vocab[UNKNOWN_TOKEN] = np.random.randint(0,len(vocab))\n",
    "weights = torch.FloatTensor(embedding_model.wv.vectors)\n",
    "embeddings = nn.Embedding.from_pretrained(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "def collate(batch):\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    for i in range(len(batch)):\n",
    "        inputs.append(torch.tensor([vocab[word] if word in vocab else vocab[UNKNOWN_TOKEN] for word in batch[i][0]]))\n",
    "        targets.append(torch.tensor([vocab[word] if word in vocab else vocab[UNKNOWN_TOKEN] for word in batch[i][1]]))\n",
    "    return pad_sequence(inputs, batch_first=True, padding_value=vocab[PAD_TOKEN]),pad_sequence(targets, batch_first=True, padding_value=vocab[PAD_TOKEN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_data_loader = DataLoader(lstm_data, batch_size=32, shuffle=True, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 50])\n",
      "torch.Size([32, 50])\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    for inputs, targets in lstm_data_loader:\n",
    "        print(inputs.shape)\n",
    "        print(targets.shape)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SoS', 'australia', 'current', 'account', 'deficit', 'shrunk', 'record', 'billion', 'dollars', 'lrb', 'billion', 'us', 'rrb', 'june', 'quarter', 'due', 'soaring', 'commodity', 'prices', 'figures', 'released', 'monday', 'showed', 'EoS']\n",
      "['SoS', 'australia', 'current', 'account', 'deficit', 'shrunk', 'record', 'billion', 'dollars', 'lrb', 'billion', 'us', 'rrb', 'june', 'quarter', 'due', 'soaring', 'commodity', 'prices', 'figures', 'released', 'monday', 'showed']\n",
      "['australia', 'current', 'account', 'deficit', 'shrunk', 'record', 'billion', 'dollars', 'lrb', 'billion', 'us', 'rrb', 'june', 'quarter', 'due', 'soaring', 'commodity', 'prices', 'figures', 'released', 'monday', 'showed', 'EoS']\n",
      "['SoS', 'least', 'two', 'people', 'killed', 'suspected', 'bomb', 'attack', 'passenger', 'bus', 'strifetorn', 'southern', 'philippines', 'monday', 'military', 'said', 'EoS']\n",
      "['SoS', 'least', 'two', 'people', 'killed', 'suspected', 'bomb', 'attack', 'passenger', 'bus', 'strifetorn', 'southern', 'philippines', 'monday', 'military', 'said']\n",
      "['least', 'two', 'people', 'killed', 'suspected', 'bomb', 'attack', 'passenger', 'bus', 'strifetorn', 'southern', 'philippines', 'monday', 'military', 'said', 'EoS']\n",
      "['SoS', 'australian', 'shares', 'closed', 'percent', 'monday', 'following', 'weak', 'lead', 'united', 'states', 'lower', 'commodity', 'prices', 'dealers', 'said', 'EoS']\n",
      "['SoS', 'australian', 'shares', 'closed', 'percent', 'monday', 'following', 'weak', 'lead', 'united', 'states', 'lower', 'commodity', 'prices', 'dealers', 'said']\n",
      "['australian', 'shares', 'closed', 'percent', 'monday', 'following', 'weak', 'lead', 'united', 'states', 'lower', 'commodity', 'prices', 'dealers', 'said', 'EoS']\n",
      "['SoS', 'south', 'korea', 'nuclear', 'envoy', 'kim', 'sook', 'urged', 'north', 'korea', 'monday', 'restart', 'work', 'disable', 'nuclear', 'plants', 'stop', 'typical', 'brinkmanship', 'negotiations', 'EoS']\n",
      "['SoS', 'south', 'korea', 'nuclear', 'envoy', 'kim', 'sook', 'urged', 'north', 'korea', 'monday', 'restart', 'work', 'disable', 'nuclear', 'plants', 'stop', 'typical', 'brinkmanship', 'negotiations']\n",
      "['south', 'korea', 'nuclear', 'envoy', 'kim', 'sook', 'urged', 'north', 'korea', 'monday', 'restart', 'work', 'disable', 'nuclear', 'plants', 'stop', 'typical', 'brinkmanship', 'negotiations', 'EoS']\n",
      "['SoS', 'south', 'korea', 'monday', 'announced', 'sweeping', 'tax', 'reforms', 'including', 'income', 'corporate', 'tax', 'cuts', 'boost', 'growth', 'stimulating', 'sluggish', 'private', 'consumption', 'business', 'investment', 'EoS']\n",
      "['SoS', 'south', 'korea', 'monday', 'announced', 'sweeping', 'tax', 'reforms', 'including', 'income', 'corporate', 'tax', 'cuts', 'boost', 'growth', 'stimulating', 'sluggish', 'private', 'consumption', 'business', 'investment']\n",
      "['south', 'korea', 'monday', 'announced', 'sweeping', 'tax', 'reforms', 'including', 'income', 'corporate', 'tax', 'cuts', 'boost', 'growth', 'stimulating', 'sluggish', 'private', 'consumption', 'business', 'investment', 'EoS']\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
