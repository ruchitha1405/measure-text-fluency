{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5ApvBzEt4wp",
        "outputId": "0a6fba7c-40de-42fd-efdb-bbd3bde43959"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n"
          ]
        }
      ],
      "source": [
        "%pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "r2CSPqEQt4wr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "DrfohprAt4wr"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from nltk import trigrams\n",
        "import numpy as np\n",
        "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
        "from nltk.lm import KneserNeyInterpolated\n",
        "from nltk import everygrams\n",
        "from nltk.lm.preprocessing import pad_both_ends\n",
        "import math\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "fxNMgATtt4wr"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('gigaword.csv', nrows=1000)\n",
        "# give the column names\n",
        "df.columns = ['summary']\n",
        "\n",
        "\n",
        "tokenized_text = [sentence.lower().split() for sentence in df['summary']]\n",
        "\n",
        "\n",
        "train_data, padded_vocab = padded_everygram_pipeline(3, tokenized_text)\n",
        "\n",
        "model = KneserNeyInterpolated(order=3)\n",
        "model.fit(train_data, padded_vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "I6U2apBAt4wr"
      },
      "outputs": [],
      "source": [
        "def calculate_A1_fluency(sentence, model):\n",
        "    if not sentence:\n",
        "        return 0\n",
        "\n",
        "    split_sentence = sentence.lower().split()\n",
        "    # Pad the sentence\n",
        "    padded_sentence = list(pad_both_ends(split_sentence, n=3))\n",
        "    # Get all the trigrams\n",
        "    trigrams_in_sentence = list(trigrams(padded_sentence))\n",
        "    # Calculate the probability of the sentence\n",
        "    sentence_log_probability = 0.0\n",
        "    for trigram in trigrams_in_sentence:\n",
        "        trigram_probability = model.score(trigram[2], (trigram[0], trigram[1]))\n",
        "        sentence_log_probability += math.log(max(trigram_probability, 1e-10))\n",
        "    return sentence_log_probability/len(trigrams_in_sentence)\n",
        "     # Calculate the product of probabilities for each trigram\n",
        "    # for trigram in sentence_trigrams:\n",
        "    #     *context, word = trigram\n",
        "    #     trigram_prob = model.score(word, context)\n",
        "    #     # Safeguard against log(0) by using a minimum probability threshold\n",
        "    #     probability *= max(trigram_prob, 1e-10)\n",
        "\n",
        "    # return probability\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "_dzhSyEWt4wr"
      },
      "outputs": [],
      "source": [
        "def getValuesOfGoodAndBad(model):\n",
        "    # get all the conditional probabilities of the model\n",
        "    conditional_probabilities = model.context_counts\n",
        "    # sort them in descending order\n",
        "    sorted_conditional_probabilities = sorted(conditional_probabilities.items(), key=lambda x: x[1], reverse=True)\n",
        "    # lowest CP of the first forty percent is assigned to  value for good\n",
        "    # highest CP of the last twenty percent is assigned to value for bad\n",
        "    good = sorted_conditional_probabilities[int(len(sorted_conditional_probabilities)*0.4)][1]\n",
        "    bad = sorted_conditional_probabilities[int(len(sorted_conditional_probabilities)*0.8)][1]\n",
        "    return good, bad\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "EsLZ77SZt4ws"
      },
      "outputs": [],
      "source": [
        "# valForGood ,valForBad = getValuesOfGoodAndBad(model)\n",
        "valForGood = 0.0000001\n",
        "valForBad =  0.000000001\n",
        "def calculate_A2_fluency(sentence, model):\n",
        "    if not sentence:\n",
        "        return 0\n",
        "    split_sentence = sentence.lower().split()\n",
        "    # Pad the sentence\n",
        "    padded_sentence = list(pad_both_ends(split_sentence, n=3))\n",
        "    # Get all the trigrams\n",
        "    trigrams_in_sentence = list(trigrams(padded_sentence))\n",
        "    # Calculate the probability of the sentence\n",
        "    fluency = 1.0/len(trigrams_in_sentence)\n",
        "    for trigram in trigrams_in_sentence:\n",
        "        trigram_probability = model.score(trigram[2], (trigram[0], trigram[1]))\n",
        "        if trigram_probability >= valForGood:\n",
        "            fluency = fluency/trigram_probability\n",
        "        elif trigram_probability <= valForBad:\n",
        "            fluency = fluency*trigram_probability\n",
        "    return fluency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "AjNRomont4ws"
      },
      "outputs": [],
      "source": [
        "df['A1_fluency_score'] = df['summary'].apply(lambda x: calculate_A1_fluency(x, model))\n",
        "df['A2_fluency_score'] = df['summary'].apply(lambda x: calculate_A2_fluency(x, model))\n",
        "\n",
        "df.to_csv('A1A2_fluency_scores.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}