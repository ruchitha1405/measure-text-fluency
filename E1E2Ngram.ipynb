{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "import math\n",
    "import datetime\n",
    "import pickle\n",
    "import json\n",
    "# import re\n",
    "from tok import Tokenizer as tk\n",
    "from queue import PriorityQueue as pq\n",
    "from scipy.stats import linregress\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCorpusName(corpusPath):\n",
    "        return corpusPath.split('/')[-1].split('.')[0]\n",
    "\n",
    "# def getModelPath(LMtype, corpusPath, n):\n",
    "#         return f'./models/{LMtype}/{getCorpusName(corpusPath)}/{n}Gram.json'\n",
    "\n",
    "def getPickleModelPath(LMtype, corpusPath, n):\n",
    "        return f'./models/{LMtype}/{getCorpusName(corpusPath)}/{n}Gram.pkl'\n",
    "\n",
    "# save model\n",
    "def saveModel(model, filename):\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(model, file)\n",
    "    return\n",
    "\n",
    "# load model\n",
    "def loadModel(filename):\n",
    "    with open(filename, 'rb') as file:\n",
    "        model = pickle.load(file)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class N_Gram_Model():\n",
    "    def __init__(self, corpusPath , n  , LMtype , modelPath = None):\n",
    "        self.corpusPath = corpusPath\n",
    "        self.n = n\n",
    "        self.frequencies = {} # n-gram: frequency\n",
    "        self.numOfNgrams = None\n",
    "        self.nGramIndices = {} # n-gram: index\n",
    "        self.normalProbabilities = None # np array, n-gram index - normal probability\n",
    "        # self.gtProbabilities = {} # n-gram: gt probability\n",
    "        # self.interpolatedProbabilities = {} # n-gram: interpolated probability\n",
    "        self.LMtype = LMtype # based on the LM type, decision will be made on which probabilities to use\n",
    "        self.modelPath = modelPath\n",
    "        self.tokenizedCorpus = None\n",
    "        self.trainData = None\n",
    "        self.testData = None\n",
    "        self.trainDataDictionary = None\n",
    "        self.unkThreshold = 3\n",
    "        self.trainDataWithUNK = None\n",
    "        self.rStar = None # r* values for gt smoothing of 3-gram model\n",
    "        self.lambda1 = None\n",
    "        self.lambda2 = None\n",
    "        self.lambda3 = None\n",
    "        # self.uniNormalProb = {}\n",
    "        # self.biNormalProb = {}\n",
    "        # self.triNormalProb = {}\n",
    "        self.uniGram = None\n",
    "        self.biGram = None\n",
    "        self.triGram = None\n",
    "        self.negligibleProb = math.pow(10, -15)\n",
    "        \n",
    "    \n",
    "    def readCorpusAndTokenize(self):\n",
    "        # read the corpus and save it in a variable\n",
    "        with open(self.corpusPath, 'r') as f:\n",
    "            corpusText = f.read()\n",
    "        # tokenize the corpus\n",
    "        self.tokenizedCorpus = self.tokenize(corpusText)\n",
    "    \n",
    "    def tokenize(self,text):\n",
    "        # tokenize the text\n",
    "        tokenizer = tk()\n",
    "        return tokenizer.Tokenize(text)\n",
    "\n",
    "    def getTrainTestDataSets(self):\n",
    "        # split the corpus into train and test sets\n",
    "        random.seed(42)\n",
    "        data = self.tokenizedCorpus\n",
    "        random.shuffle(data)\n",
    "        self.trainData = data[1000:]\n",
    "        self.testData = data[:1000]\n",
    "    \n",
    "    def getTrainDataDictionary(self):\n",
    "        # get the dictionary of the train data\n",
    "        self.trainDataDictionary = self.getDataDictionary(self.trainData)\n",
    "    \n",
    "    def getTrainDataWithUNK(self):\n",
    "        # replace the rare words with <unk>\n",
    "        self.trainDataWithUNK = self.replaceRareWords(self.trainData , self.trainDataDictionary)\n",
    "\n",
    "    def setUniGramBiGramTriGramModelsForInterpolation(self , uniGram , biGram , triGram):\n",
    "        self.uniGram = uniGram\n",
    "        self.biGram = biGram\n",
    "        self.triGram = triGram\n",
    "    \n",
    "    def train(self):\n",
    "        # train the model on the train set\n",
    "        # calculate the frequencies and probabilities of all the n-grams\n",
    "        self.calculateNgramFrequencies(self.trainDataWithUNK)\n",
    "        # get the number of n-grams\n",
    "        self.numOfNgrams = len(self.frequencies)\n",
    "        if (self.LMtype == \"n\"):\n",
    "            # 2. calculate the normal probabilities\n",
    "            # self.getNormalProbabilities()\n",
    "            pass\n",
    "        if (self.LMtype == \"g\"):\n",
    "            # 3. calculate the gt probabilities\n",
    "            self.goodTuringSmoothing()\n",
    "        if (self.LMtype == \"i\"):\n",
    "            # calculate the interpolated probabilities\n",
    "            self.interpolation()\n",
    "        # 5. save the model\n",
    "        # self.saveModel()\n",
    "        return\n",
    "    \n",
    "    def calculateNgramFrequencies(self, dataWithUNK):\n",
    "        n = self.n\n",
    "        dataWithStartEndSymbols = self.addStartEndSymbols(dataWithUNK , n)\n",
    "        index = 0\n",
    "        for sentence in dataWithStartEndSymbols:\n",
    "            for i in range((len(sentence) - n) + 1):\n",
    "                ngram = tuple(sentence[i:i+n])\n",
    "                if n == 1:\n",
    "                    ngram = sentence[i]\n",
    "                if ngram in self.frequencies:\n",
    "                    self.frequencies[ngram] += 1\n",
    "                else:\n",
    "                    self.frequencies[ngram] = 1\n",
    "                    self.nGramIndices[ngram] = index\n",
    "                    index += 1\n",
    "        return\n",
    "    def getProbabilityOfUserInput(self , userInput):\n",
    "        # get the probability of the text entered by the user\n",
    "        # tokenize the user input\n",
    "        tokens = self.tokenize(userInput)\n",
    "        # # Flatten the 2D list using the sum function\n",
    "        one_d_tokens = sum(tokens, [])\n",
    "        # replace the rare words with <unk>\n",
    "        replacedSentence = self.replaceRareWords([one_d_tokens] , self.trainDataDictionary)\n",
    "        # add start and end symbols to the user input\n",
    "        sentence = self.addStartEndSymbols([replacedSentence[0]] , self.n)[0]\n",
    "        return math.exp(self.getLogProbabilityOfSentence(sentence))\n",
    "\n",
    "\n",
    "    def getLogProbabilityOfSentence(self , sentence):\n",
    "        # calculate the probability of the sentence\n",
    "        initProbability = 1\n",
    "        initLog = math.log(initProbability)\n",
    "        logProbability = float(initLog)\n",
    "        for i in range((len(sentence) - self.n) + 1):\n",
    "            ngram = tuple(sentence[i:i+self.n])\n",
    "            if self.n == 1:\n",
    "                ngram = sentence[i]\n",
    "            probability = self.getProbabilityOfNgram(ngram) # p(wn/w1...wn-1)\n",
    "            logProbability += math.log(probability)\n",
    "        return logProbability\n",
    "    \n",
    "    \n",
    "    def getProbabilityOfNgram(self, ngram):\n",
    "        # get the probability of the ngram depending on the LM type p(wn/w1...wn-1)\n",
    "        if self.LMtype == \"n\":\n",
    "            # if ngram not in self.frequencies:\n",
    "            #     return self.negligibleProb\n",
    "            # else:\n",
    "            # get normal probability\n",
    "            return self.getNormalProbabilityOfNgram(ngram)\n",
    "        elif self.LMtype == \"g\":\n",
    "            # get gt probability of the 3-gram\n",
    "            if len(ngram) != 3:\n",
    "                print(\"The gt probability is only supported for trigram model.\")\n",
    "                exit()\n",
    "            return self.getGtProbabilityOfTrigram(ngram)\n",
    "        elif self.LMtype == \"i\":\n",
    "            # get interpolated probability of the 3-gram\n",
    "            if len(ngram) != 3:\n",
    "                print(\"The interpolated probability is only supported for trigram model.\")\n",
    "                exit()\n",
    "            return self.getInterpolatedProbabilityOfTrigram(ngram)\n",
    "    \n",
    "    def evaluate(self , data):\n",
    "        # evaluate the 3-gram model on the data set\n",
    "        if (self.n != 3):\n",
    "            print(\"The evaluation is only supported for 3-gram model.\")\n",
    "            exit()\n",
    "        # calculate the perplexity of the every sentence in the data set\n",
    "        # calculate the avg perplexity of the data set\n",
    "        perplexity = {}\n",
    "        for sentence in data:\n",
    "            perplexity[tuple(sentence)] = self.perplexity(sentence)\n",
    "        avgPerplexity = sum(perplexity.values()) /float(len(perplexity))\n",
    "        return perplexity, avgPerplexity\n",
    "    \n",
    "    def perplexity(self, sentence):\n",
    "        # calculate the perplexity of the sentence using the 3-gram model\n",
    "        if (self.n != 3):\n",
    "            print(\"The perplexity is only supported for 3-gram model.\")\n",
    "            exit()\n",
    "        # replace the rare words with <unk>\n",
    "        replacedSentence = self.replaceRareWords([sentence] , self.trainDataDictionary)\n",
    "        # add start and end symbols to the user input\n",
    "        newSentence = self.addStartEndSymbols([replacedSentence[0]] , self.n)[0]\n",
    "        # probabilities = self.decideProbabilities() # based on the LM type\n",
    "        logProbOfSentence = self.getLogProbabilityOfSentence(newSentence)\n",
    "        numOfWordsIncludingEnd = len(newSentence)- 2\n",
    "        # if probOfSentence == 0:\n",
    "        #     print(\"The probability of the sentence is zero.\", newSentence)\n",
    "        #     exit()\n",
    "        # if probOfSentence < 0:\n",
    "        #     print(\"The probability of the sentence is negative.\", newSentence)\n",
    "        #     exit()\n",
    "        return math.exp(logProbOfSentence * (-1 / float(numOfWordsIncludingEnd)))\n",
    "    \n",
    "    def getNormalProbabilityOfNgram(self , ngram):\n",
    "        # calculate the normal probability p(Wn/W1...Wn-1)\n",
    "        if ngram not in self.frequencies:\n",
    "            return self.negligibleProb\n",
    "        if (self.n == 1):\n",
    "            denominator = 0\n",
    "            for unigram in self.frequencies:\n",
    "                denominator += self.frequencies[unigram]\n",
    "            return self.frequencies[ngram] / float(denominator)\n",
    "        denominator = 0\n",
    "        for ngram2 in self.frequencies:\n",
    "            if ngram[:-1] == ngram2[:-1]:\n",
    "                denominator += self.frequencies[ngram2]\n",
    "        if(denominator == 0): # it never happens\n",
    "            print(\"denominator is zero while calculating the normal probability for ngram: \" , ngram)\n",
    "            exit()\n",
    "        return self.frequencies[ngram] / float(denominator)\n",
    "\n",
    "    def getAllNormalProbabilities(self):\n",
    "        # calculate the normal probabilities p(Wn/W1...Wn-1) for all n-grams\n",
    "        self.normalProbabilities = np.zeros(self.numOfNgrams).astype(float)\n",
    "        for ngram in self.nGramIndices:\n",
    "            self.normalProbabilities[self.nGramIndices[ngram]] = self.getNormalProbabilityOfNgram(ngram)\n",
    "        return\n",
    "\n",
    "    def goodTuringSmoothing(self):\n",
    "        # calculate the gtProbabilities p(w3/w1w2)\n",
    "        # 1. calculate Nr's\n",
    "        maxFreq = max(self.frequencies.values())\n",
    "        freqOfFreq = np.zeros(maxFreq+1)\n",
    "        for ngram in self.frequencies:\n",
    "            freqOfFreq[self.frequencies[ngram]] += 1\n",
    "        # 2. fit line to log(r) and log(Zr)  to get log(Nr) = a + b * log(r)\n",
    "        x = np.array([])\n",
    "        y = np.array([])\n",
    "        q = 0\n",
    "        for r in range(1,maxFreq+1):\n",
    "            if freqOfFreq[r] > 0:\n",
    "                x = np.append(x, math.log(r))\n",
    "                    # y = np.append(y, math.log(freqOfFreq[r]))\n",
    "                # calculate Zr = 2Nr/(t-q), where q,r,t hree consecutive subscripts with non-zero counts Nq, Nr, Nt\n",
    "                Zr = None\n",
    "                if r == 1:\n",
    "                    # t is next non-zero Nt after Nr\n",
    "                    temp = 2\n",
    "                    while freqOfFreq[temp] == 0:\n",
    "                        temp += 1\n",
    "                    t = temp\n",
    "                    Zr = 2 * freqOfFreq[r] / float(t - q)\n",
    "                if r == maxFreq:\n",
    "                    Zr = freqOfFreq[r] / float(r - q)\n",
    "                if r != 1 and r != maxFreq:\n",
    "                    temp = r + 1\n",
    "                    while freqOfFreq[temp] == 0:\n",
    "                        temp += 1\n",
    "                    t = temp\n",
    "                    Zr = 2 * freqOfFreq[r] / float(t - q)\n",
    "                y = np.append(y, math.log(Zr))\n",
    "                q = r\n",
    "                    \n",
    "        line = linregress(x, y)\n",
    "        a = line.intercept\n",
    "        b = line.slope\n",
    "        # 3. calculate r* = (r+1) * (S(Nr+1) / S(Nr)) for all r , S(Nr) = exp(a + b * log(r)) for given r\n",
    "        # initialize rStar with float zeros\n",
    "        self.rStar = np.zeros(maxFreq+1).astype(float)\n",
    "        for r in range(0,maxFreq+1):\n",
    "            self.rStar[r] = (r+1) * self.smoothedNr(r+1, a, b) / float(self.smoothedNr(r, a, b))\n",
    "        # # 4. calculate the gt probabilities p(w3/w1w2) where freq(w1w2w3) > 0\n",
    "        # for ngram in self.frequencies:\n",
    "        #     self.gtProbabilities[ngram] = self.getGtProbabilityOfTrigram(ngram)\n",
    "        return\n",
    "    \n",
    "    def getGtProbabilityOfTrigram(self , trigram):\n",
    "        # calculate the gt probability P(w3/w1w2)\n",
    "        if trigram not in self.frequencies:\n",
    "            count = 0\n",
    "        else :\n",
    "            count = self.frequencies[trigram]\n",
    "        countStar = self.rStar[count] # Count*(w1w2w3)\n",
    "        denominator = 0 # sum of all countStars for w1w2wi where i is in the vocabulary\n",
    "        for word in self.trainDataDictionary:\n",
    "            newTriGram = tuple([trigram[0],trigram[1],word])\n",
    "            if newTriGram in self.frequencies:\n",
    "                # r > 0\n",
    "                r = self.frequencies[newTriGram]\n",
    "                denominator += self.rStar[r]\n",
    "            else:\n",
    "                # r = 0\n",
    "                denominator += self.rStar[0]\n",
    "        if denominator == 0:\n",
    "            print(\"The denominator is zero while calculating the gt probability for trigram: \" , trigram)\n",
    "            exit()\n",
    "        return countStar / float(denominator)\n",
    "    \n",
    "    def smoothedNr(self, r, intercept, slope):\n",
    "        # calculate the smoothed Nr\n",
    "        if r == 0:\n",
    "            return 1\n",
    "        return math.exp(intercept + slope * math.log(r))\n",
    "\n",
    "    def interpolation(self ):\n",
    "        # calculate the interpolatedProbabilities of the 3-gram model p(w3/w1w2)\n",
    "        # train the unigram, bigram and trigram models\n",
    "        if self.uniGram == None and self.biGram == None and self.triGram == None:\n",
    "            self.uniGram = N_Gram_Model(self.corpusPath , 1 , \"n\")\n",
    "            self.uniGram.trainDataWithUNK = self.trainDataWithUNK\n",
    "            self.uniGram.train()\n",
    "            self.biGram = N_Gram_Model(self.corpusPath , 2 , \"n\")\n",
    "            self.biGram.trainDataWithUNK = self.trainDataWithUNK\n",
    "            self.biGram.train()\n",
    "            # self.triGram.train()\n",
    "            # self.triGram = self\n",
    "            self.triGram = N_Gram_Model(self.corpusPath , 3 , \"n\")\n",
    "            self.triGram.trainDataWithUNK = self.trainDataWithUNK\n",
    "            # self.triGram.train()\n",
    "            self.triGram.frequencies = self.frequencies\n",
    "            self.triGram.numOfNgrams = self.numOfNgrams\n",
    "            self.triGram.nGramIndices = self.nGramIndices\n",
    "        # self.uniNormalProb = uniNormalProb\n",
    "        # self.biNormalProb = biNormalProb\n",
    "        # self.triNormalProb = triNormalProb\n",
    "        if (self.n != 3):\n",
    "            print(\"The interpolation is only supported for trigram model.\")\n",
    "            exit()\n",
    "        self.lambda1, self.lambda2, self.lambda3 = self.getLambdasOfTrigramInterpolation()\n",
    "        # for trigram in self.frequencies:\n",
    "        #     self.interpolatedProbabilities[trigram] = self.getInterpolatedProbabilityOfTrigram(trigram , self.lambda1, self.lambda2, self.lambda3 , uniNormalProb, biNormalProb, triNormalProb)\n",
    "        return\n",
    "\n",
    "    def getInterpolatedProbabilityOfTrigram(self , trigram):\n",
    "        # calculate the interpolated probability P(w3/w1w2)\n",
    "        bigram = tuple(trigram[1:3])\n",
    "        # unigram = tuple([trigram[2]])\n",
    "        unigram = trigram[2]\n",
    "        uniNormProb = self.uniGram.getNormalProbabilityOfNgram(unigram)\n",
    "        biNormProb = self.biGram.getNormalProbabilityOfNgram(bigram)\n",
    "        triNormProb = self.triGram.getNormalProbabilityOfNgram(trigram)\n",
    "        # if uniProb == 0:\n",
    "        #     uniProb = self.negligibleProb\n",
    "        # if biProb == 0:\n",
    "        #     biProb = self.negligibleProb\n",
    "        # if triProb == 0:\n",
    "        #     triProb = self.negligibleProb\n",
    "        return self.lambda1 * uniNormProb + self.lambda2 * biNormProb + self.lambda3 * triNormProb\n",
    "\n",
    "    def getLambdasOfTrigramInterpolation(self):\n",
    "        # calculate the lambdas for the trigram interpolation\n",
    "        lambda1 = lambda2 = lambda3 = 0.0\n",
    "        uniFreq = self.uniGram.frequencies\n",
    "        biFreq = self.biGram.frequencies\n",
    "        triFreq = self.triGram.frequencies\n",
    "        x = [0, 0, 0]\n",
    "        for triGram in self.frequencies:\n",
    "            # uniTuple = tuple([triGram[2]])\n",
    "            (t1, t2, t3) = triGram\n",
    "            # uniTuple = triGram[2]\n",
    "            uniT3 = t3\n",
    "            # uniT3NormalProb = self.uniGram.getNormalProbabilityOfNgram(uniT3)\n",
    "            # denominator0 = float((uniFreq[uniT3Tuple]/uniNormalProb) - 1)\n",
    "            denominator0 = sum(uniFreq.values()) - 1\n",
    "            if denominator0 == 0:\n",
    "                x[0] = 0\n",
    "            else:\n",
    "                x[0] = (uniFreq[uniT3] - 1)/denominator0\n",
    "            # biTuple = tuple(triGram[1:3])\n",
    "            # biNormalProb = self.biGram.getNormalProbabilityOfNgram(biTuple)\n",
    "            # denominator1 = float((biFreq[biTuple]/biNormalProb) - 1)\n",
    "            uniT2 = t2\n",
    "            biT2T3 = (t2, t3)\n",
    "            denominator1 = float((uniFreq[uniT2] - 1))\n",
    "            if denominator1 == 0:\n",
    "                x[1] = 0\n",
    "            else:\n",
    "                x[1] = (biFreq[biT2T3] - 1)/denominator1\n",
    "            # triNormalProb = self.triGram.getNormalProbabilityOfNgram(triGram)\n",
    "            # denominator2 = float((triFreq[triGram]/triNormalProb) - 1)\n",
    "            biT1T2 = (t1, t2)\n",
    "            denominator2 = float((biFreq[biT1T2] - 1))\n",
    "            if denominator2 == 0:\n",
    "                x[2] = 0\n",
    "            else:\n",
    "                x[2] = (triFreq[triGram] - 1)/denominator2\n",
    "            maxIndex = x.index(max(x))\n",
    "            if maxIndex == 0:\n",
    "                lambda1 += triFreq[triGram]\n",
    "            elif maxIndex == 1:\n",
    "                lambda2 += triFreq[triGram]\n",
    "            elif maxIndex == 2:\n",
    "                lambda3 += triFreq[triGram]\n",
    "        # normalize the lambdas\n",
    "        total = lambda1 + lambda2 + lambda3\n",
    "        if total <=0 :\n",
    "            return 0.33, 0.33, 0.34 #assigning default probabilities as backup\n",
    "        lambda1 = lambda1 / total\n",
    "        lambda2 = lambda2 / total\n",
    "        lambda3 = lambda3 / total   \n",
    "        return lambda1, lambda2, lambda3\n",
    "\n",
    "    def getDataDictionary(self, data):\n",
    "        # get the dictionary of the data which is already tokenized\n",
    "        dic = {}\n",
    "        for sentence in data:\n",
    "            for word in sentence:\n",
    "                if word in dic:\n",
    "                    dic[word] += 1\n",
    "                else:\n",
    "                    dic[word] = 1\n",
    "        return dic\n",
    "\n",
    "    def replaceRareWords(self , data , dictionary):\n",
    "        # replace the rare words with <unk>\n",
    "        threshold = self.unkThreshold\n",
    "        dataWithUNK = data.copy()\n",
    "        for sentence in range(len(dataWithUNK)):\n",
    "            for word in range(len(dataWithUNK[sentence])):\n",
    "                if dataWithUNK[sentence][word] not in dictionary or dictionary[dataWithUNK[sentence][word]] < threshold:\n",
    "                    dataWithUNK[sentence][word] = '<UNK>'\n",
    "        return dataWithUNK\n",
    "    \n",
    "    def addStartEndSymbols(self , data , n):\n",
    "            # add start and end symbols to the data\n",
    "            dataWithStartEndSymbols = data.copy()\n",
    "            for sentence in range(len(data)):\n",
    "                if n > 1:\n",
    "                    for i in range(n-1):\n",
    "                        dataWithStartEndSymbols[sentence].insert(0 , '<START>')\n",
    "                dataWithStartEndSymbols[sentence].append('<END>')\n",
    "                if n == 1:\n",
    "                    dataWithStartEndSymbols[sentence].insert(0 , '<START>')\n",
    "            return dataWithStartEndSymbols\n",
    "    \n",
    "    def fluencyWithA1(self, input):\n",
    "        # get the probability of the text entered by the user\n",
    "        # tokenize the user input\n",
    "        tokens = self.tokenize(input)\n",
    "        # # Flatten the 2D list using the sum function\n",
    "        one_d_tokens = sum(tokens, [])\n",
    "        # replace the rare words with <unk>\n",
    "        replacedSentence = self.replaceRareWords([one_d_tokens] , self.trainDataDictionary)\n",
    "        # add start and end symbols to the user input\n",
    "        sentence = self.addStartEndSymbols([replacedSentence[0]] , self.n)[0]\n",
    "        m = len(replacedSentence[0])\n",
    "        fluencyScore = self.getLogProbabilityOfSentence(sentence)/m\n",
    "        return fluencyScore\n",
    "\n",
    "\n",
    "    def fluencyWithA2(self,input):\n",
    "        # get the probability of the text entered by the user\n",
    "        # tokenize the user input\n",
    "        tokens = self.tokenize(input)\n",
    "        # # Flatten the 2D list using the sum function\n",
    "        one_d_tokens = sum(tokens, [])\n",
    "        # replace the rare words with <unk>\n",
    "        replacedSentence = self.replaceRareWords([one_d_tokens] , self.trainDataDictionary)\n",
    "        # add start and end symbols to the user input\n",
    "        sentence = self.addStartEndSymbols([replacedSentence[0]] , self.n)[0]\n",
    "        m = len(replacedSentence[0])\n",
    "        fluencyScore = 1.0/m\n",
    "        valForGood , valForBad = self.getValuesGoodBad()\n",
    "        for i in range(self.n-1,self.n+m):\n",
    "            # get n-grams from sentence\n",
    "            ngram = tuple(sentence[i:i+self.n])\n",
    "            if self.n == 1:\n",
    "                ngram = sentence[i]\n",
    "            nGramProbabilty = math.exp(self.getLogProbabilityOfSentence(ngram))\n",
    "            if nGramProbabilty >= valForGood:\n",
    "                fluencyScore = fluencyScore/nGramProbabilty\n",
    "            elif nGramProbabilty <= valForBad:\n",
    "                fluencyScore = fluencyScore * nGramProbabilty\n",
    "        return fluencyScore\n",
    "    \n",
    "    def getValuesGoodBad(self):\n",
    "        self.getAllNormalProbabilities()\n",
    "        if self.normalProbabilities is not None:\n",
    "            CPs = self.normalProbabilities\n",
    "        else:\n",
    "            print(\"getAllNormalProbabilities did n't update  self.normalProbabilities\")\n",
    "        CPs = np.sort(CPs)\n",
    "        goodIndex = 6* len(CPs)/10\n",
    "        badIndex = 2* len(CPs)/10\n",
    "        return CPs[goodIndex], CPs[badIndex]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Models\n",
    "corpus = \"./corpus/ggword.txt\"\n",
    "### No smoothing\n",
    "def getTrainedModel(model : N_Gram_Model):\n",
    "    # read and tokenize the corpus\n",
    "    model.readCorpusAndTokenize()\n",
    "    # get train and test data\n",
    "    model.getTrainTestDataSets()\n",
    "    # get trainDataDictionary\n",
    "    model.getTrainDataDictionary()\n",
    "    # get trainDataReplacedByUnk\n",
    "    model.getTrainDataWithUNK()\n",
    "    # train the model\n",
    "    model.train()\n",
    "    return model\n",
    "# N = 1,2,3\n",
    "# create a 3-gram model of the given corpus of given type\n",
    "\n",
    "uni1Model = N_Gram_Model(corpus,1 , \"n\", getPickleModelPath(\"n\",corpus,1))\n",
    "uni1Model = getTrainedModel(uni1Model)\n",
    "saveModel(uni1Model,uni1Model.modelPath)\n",
    "bi1Model = N_Gram_Model(corpus,2 , \"n\", getPickleModelPath(\"n\",corpus,2))\n",
    "bi1Model = getTrainedModel(bi1Model)\n",
    "saveModel(bi1Model,bi1Model.modelPath)\n",
    "tri1Model = N_Gram_Model(corpus,3 , \"n\", getPickleModelPath(\"n\",corpus,3))\n",
    "tri1Model = getTrainedModel(tri1Model)\n",
    "saveModel(tri1Model,tri1Model.modelPath)\n",
    "# triGramModel = getTrainedModel(model)\n",
    "uni2Model = N_Gram_Model(corpus,1 , \"g\", getPickleModelPath(\"g\",corpus,1))\n",
    "uni2Model = getTrainedModel(uni2Model)\n",
    "saveModel(uni2Model,uni2Model.modelPath)\n",
    "bi2Model = N_Gram_Model(corpus,2 , \"g\", getPickleModelPath(\"g\",corpus,2))\n",
    "bi2Model = getTrainedModel(bi2Model)\n",
    "saveModel(bi2Model,bi2Model.modelPath)\n",
    "tri2Model = N_Gram_Model(corpus,3 , \"g\", getPickleModelPath(\"g\",corpus,3))\n",
    "tri2Model = getTrainedModel(tri2Model)\n",
    "saveModel(tri2Model,tri2Model.modelPath)\n",
    "# save the current tri-gram model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take command line arguments\n",
    "commandLineLMtype = sys.argv[1]\n",
    "commandLineCorpusPath = sys.argv[2]\n",
    "# only i,n,g types are allowed\n",
    "if commandLineLMtype not in [\"i\",\"n\",\"g\"]:\n",
    "    print(\"Invalid language model type\")\n",
    "    sys.exit()\n",
    "# load model \n",
    "triGramModel = loadModel(getPickleModelPath(commandLineLMtype,commandLineCorpusPath , 3))\n",
    "# User Prompt\n",
    "while True:\n",
    "    try :\n",
    "        inputSentence = input(\"input sentence: \")\n",
    "        if(inputSentence == \"\"):\n",
    "            raise Exception(\"Empty input!\")\n",
    "        print(\"A1 fluency score: \", triGramModel.fluencyWithA1(inputSentence))\n",
    "        print(\"A2 fluency score: \", triGramModel.fluencyWithA2(inputSentence))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"Please enter a valid sentence\")\n",
    "    finally:\n",
    "        print(\"Do you want to continue? (y/n)\")\n",
    "        if(input() == 'n'):\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
