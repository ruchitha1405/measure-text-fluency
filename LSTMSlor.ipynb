{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jazzblazzer/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/jazzblazzer/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import nltk\n",
    "import string\n",
    "from gensim.models import word2vec\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "import torch.nn as nn\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "nltk.download('stopwords')\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import re\n",
    "from torchtext.vocab import GloVe\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['document', 'summary'],\n",
      "        num_rows: 3803957\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['document', 'summary'],\n",
      "        num_rows: 189651\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['document', 'summary'],\n",
      "        num_rows: 1951\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"gigaword\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_TOKEN = \"<s>\"\n",
    "END_TOKEN = \"</s>\"\n",
    "UNKNOWN_TOKEN = \"<unk>\"\n",
    "PAD_TOKEN = \"<pad>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['<s>', 'japan', \"'s\", 'nec', 'corp', 'and', 'unk', 'computer', 'corp', 'of', 'the', 'united', 'states', 'said', 'wednesday', 'they', 'had', 'agreed', 'to', 'join', 'forces', 'in', 'supercomputer', 'sales'], ['japan', \"'s\", 'nec', 'corp', 'and', 'unk', 'computer', 'corp', 'of', 'the', 'united', 'states', 'said', 'wednesday', 'they', 'had', 'agreed', 'to', 'join', 'forces', 'in', 'supercomputer', 'sales', '</s>'])\n"
     ]
    }
   ],
   "source": [
    "class LSTMDataset:\n",
    "    def __init__(self, dataset, tokenizer, max_length):\n",
    "        self.dataset = dataset['document']\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        tokenized_sentence = self.tokenizer(re.sub(r'[^\\w\\s\\']', '',self.dataset[idx].lower()))\n",
    "        return [START_TOKEN]+tokenized_sentence,tokenized_sentence+[END_TOKEN]\n",
    "\n",
    "        \n",
    "    \n",
    "    \n",
    "lstm_data = LSTMDataset(dataset['test'],nltk.word_tokenize, 100)\n",
    "print(lstm_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_train_data = []\n",
    "for i in range(len(lstm_data)):\n",
    "    embed_train_data.append(lstm_data[i][0]+[END_TOKEN])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(embed_train_data, vector_size=300, window=5, min_count=3, workers=4)\n",
    "model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = Word2Vec.load(\"word2vec.model\")\n",
    "vocab = embedding_model.wv.key_to_index\n",
    "#change this to random later\n",
    "vocab[PAD_TOKEN] = len(vocab)-1\n",
    "vocab[UNKNOWN_TOKEN] = np.random.randint(0,len(vocab))\n",
    "weights = torch.FloatTensor(embedding_model.wv.vectors)\n",
    "embeddings = nn.Embedding.from_pretrained(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def collate(batch):\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    for i in range(len(batch)):\n",
    "        inputs.append(torch.tensor([vocab[word] if word in vocab else vocab[UNKNOWN_TOKEN] for word in batch[i][0]]))\n",
    "        targets.append(torch.tensor([vocab[word] if word in vocab else vocab[UNKNOWN_TOKEN] for word in batch[i][1]]))\n",
    "    return pad_sequence(inputs, batch_first=True, padding_value=vocab[PAD_TOKEN]),pad_sequence(targets, batch_first=True, padding_value=vocab[PAD_TOKEN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_data_loader = DataLoader(lstm_data, batch_size=32, shuffle=True, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2666\n"
     ]
    }
   ],
   "source": [
    "class LM(nn.Module):\n",
    "    def __init__(self,embedding,batch_size, hidden_size, num_layers,output_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.output_size=output_size\n",
    "        self.batch_size =batch_size\n",
    "        self.embedding=  embedding\n",
    "        self.LSTM = nn.LSTM(input_size = embedding.weight.shape[1],hidden_size= hidden_size, num_layers= num_layers,batch_first=True)\n",
    "        self.output = torch.nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x= self.embedding(x)\n",
    "        x, _ = self.LSTM(x)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "print(embeddings.weight.shape[0])\n",
    "\n",
    "LanguageModel = LM(embedding=embeddings,\n",
    "                batch_size=32,\n",
    "               hidden_size=512,\n",
    "               num_layers=2,\n",
    "               output_size=embeddings.weight.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if (config['lm']['train']):\n",
    "#     LanguageModel = LM(embedding=embedding,\n",
    "#                         batch_size=config['lm']['batch_size'],\n",
    "#                         hidden_size=config['lm']['hidden_size'],\n",
    "#                         num_layers=config['lm']['num_layers'],\n",
    "#                         output_size=embedding.weight.shape[0])\n",
    "#     LanguageModel.to(device)\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "#     optimizer = torch.optim.Adam(LanguageModel.parameters(), lr=config['elmo']['lr'])\n",
    "#     train_loss=[]\n",
    "#     test_loss=[]\n",
    "#     min_val_loss=1e9\n",
    "#     for e in tqdm(range(config['elmo']['epochs'])):\n",
    "#         trl,tel=0,0\n",
    "        \n",
    "#         c=0\n",
    "#         for batch in tqdm(TrainDataloader):\n",
    "#             #N,L\n",
    "#             x, y= batch\n",
    "#             x, y = x.to(device), y.to(device)\n",
    "#             # N, L, C\n",
    "#             y_pred= LanguageModel(x)\n",
    "#             y_pred.permute(0,2,1)\n",
    "#             trloss = criterion(y_pred,y)\n",
    "#             optimizer.zero_grad()\n",
    "#             trloss.backward()\n",
    "#             optimizer.step()\n",
    "            \n",
    "#             # tre/=len(sentence)\n",
    "#             trl+=trloss.item()      \n",
    "#         trl/=len(TrainDataloader)\n",
    "#         train_loss.append(trl)\n",
    "#         c=0\n",
    "#         for batch in tqdm(ValDataloader):\n",
    "#             with torch.no_grad():\n",
    "#                 #N,L\n",
    "#                 x, y= batch\n",
    "#                 x, y = x.to(device), y.to(device)\n",
    "#                 # N, L, C\n",
    "#                 y_pred= LanguageModel(x)\n",
    "#                 y_pred.permute(0,2,1)\n",
    "#                 teloss = criterion(y_pred,y)\n",
    "#                 tel+=teloss.item()\n",
    "#         tel/=len(ValDataloader)\n",
    "#         # tee/=len(val_dataset)\n",
    "#         test_loss.append(tel)\n",
    "#         if(tel<min_val_loss):\n",
    "#             torch.save(LanguageModel,f\"../model/elmo_{config['dataset']['name']}.pt\")\n",
    "#     plt.plot(train_loss,label='train')\n",
    "#     plt.plot(test_loss,label='validation')  \n",
    "#     plt.legend()\n",
    "#     plt.savefig(f\"LM_{config['lm']['path']}.png\")\n",
    "# else:\n",
    "#     LanguageModel = torch.load(config['lm']['path']) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5401.421875\n",
      "4692.990234375\n",
      "5370.6826171875\n",
      "4287.0615234375\n",
      "4154.22314453125\n",
      "4113.15966796875\n",
      "4478.81298828125\n",
      "5299.26220703125\n",
      "4309.212890625\n",
      "4660.97412109375\n",
      "3949.318359375\n",
      "4836.50390625\n",
      "4458.4072265625\n",
      "3961.369384765625\n",
      "4316.95458984375\n",
      "4459.82470703125\n",
      "4739.6162109375\n",
      "3695.995849609375\n",
      "3272.74560546875\n",
      "4069.689697265625\n",
      "4324.24609375\n",
      "4058.249267578125\n",
      "4226.99658203125\n",
      "3933.4912109375\n",
      "3661.6982421875\n",
      "3441.4990234375\n",
      "3241.19287109375\n",
      "3984.089111328125\n",
      "3611.994873046875\n",
      "3344.669189453125\n",
      "4199.4814453125\n",
      "3783.490234375\n",
      "3568.78125\n",
      "3824.4677734375\n",
      "3109.9716796875\n",
      "2951.7529296875\n",
      "3098.111328125\n",
      "3633.8779296875\n",
      "3710.006591796875\n",
      "3544.99755859375\n",
      "3935.226806640625\n",
      "3941.65771484375\n",
      "3216.423828125\n",
      "3711.536376953125\n",
      "3791.171875\n",
      "2800.38427734375\n",
      "3238.11865234375\n",
      "3808.598876953125\n",
      "3554.9814453125\n",
      "2922.981689453125\n",
      "3933.182373046875\n",
      "3088.416015625\n",
      "2712.734619140625\n",
      "2987.0380859375\n",
      "2518.781494140625\n",
      "3816.980224609375\n",
      "3768.798095703125\n",
      "3060.937255859375\n",
      "3719.266845703125\n",
      "3351.177734375\n",
      "3416.410400390625\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "\n",
    "# Lists to store training and validation losses\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "LanguageModel.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(LanguageModel.parameters(), lr=0.9)\n",
    "c = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    LanguageModel.train()\n",
    "    epoch_train_loss = 0.0\n",
    "    for batch in lstm_data_loader:\n",
    "        inputs, labels = batch\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = LanguageModel(inputs)\n",
    "        outputs = outputs.view(-1, outputs.shape[-1])\n",
    "        labels = labels.view(-1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_train_loss += loss.item()\n",
    "        print(loss.item())\n",
    "        c-=1\n",
    "        if c < 0:\n",
    "            break\n",
    "    epoch_train_loss /= len(lstm_data_loader)\n",
    "    train_loss.append(epoch_train_loss)\n",
    "    LanguageModel.eval()\n",
    "    epoch_val_loss = 0.0\n",
    "    \n",
    "    # Disable this part of code for now\n",
    "    # with torch.no_grad():\n",
    "    #     for batch in val_loader:\n",
    "    #         inputs, labels = batch\n",
    "    #         inputs, labels = inputs.to(device), labels.to(device)\n",
    "    #         outputs = model(inputs)\n",
    "    #         loss = criterion(outputs, labels)\n",
    "    #         epoch_val_loss += loss.item()\n",
    "    # epoch_val_loss /= len(val_loader)\n",
    "    # val_loss.append(epoch_val_loss)\n",
    "    \n",
    "    # if epoch_val_loss < min_val_loss:\n",
    "    #     torch.save(model.state_dict(), f\"model_epoch_{epoch}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(LanguageModel, \"model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SLOR(sentence,embedding_model,vocab,LanguageModel):\n",
    "    sentence = [START_TOKEN]+nltk.word_tokenize(re.sub(r'[^\\w\\s\\']', '',sentence.lower()))\n",
    "    labels = sentence[1:]+[END_TOKEN]\n",
    "    sentence_old = [word if word in vocab else vocab[UNKNOWN_TOKEN] for word in sentence]\n",
    "    sentence = torch.tensor([vocab[word] if word in vocab else vocab[UNKNOWN_TOKEN] for word in sentence]).squeeze(0)\n",
    "    labels = torch.tensor([vocab[word] if word in vocab else vocab[UNKNOWN_TOKEN] for word in labels]).squeeze(0)\n",
    "    LanguageModel.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred= LanguageModel(sentence)\n",
    "        y_pred = y_pred.squeeze(0)\n",
    "    y_pred = nn.functional.log_softmax(y_pred,dim=1)\n",
    "    \n",
    "    prob_sent = 1\n",
    "    for i in range(y_pred.shape[0]):\n",
    "        prob_sent += y_pred[i][labels[i]]\n",
    "    tot_words = 0\n",
    "    for keys in vocab.keys():\n",
    "        tot_words+=embedding_model.wv.get_vecattr(keys,'count')\n",
    "    unigram_prob = 1\n",
    "    for i in range(len(sentence)):\n",
    "        unigram_prob *= embedding_model.wv.get_vecattr(sentence_old[i],'count')/tot_words\n",
    "    return (prob_sent)/(len(sentence)+1) - np.log(unigram_prob)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-3150.1748)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SLOR(dataset['test']['document'][0],embedding_model,vocab,LanguageModel)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
